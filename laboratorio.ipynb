{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sys import path\n",
    "path.append('../')\n",
    "import hsim.core.pymulate as pym\n",
    "from hsim.core.chfsm import CHFSM, Transition, State\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simpy import AnyOf\n",
    "from copy import deepcopy\n",
    "from random import choices,seed,normalvariate, expovariate\n",
    "from hsim.core.stores import Store, Box       \n",
    "from scipy import stats\n",
    "import dill\n",
    "import hsim.core.utils as utils\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import deque \n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def plot_service_times(service_times_log):\n",
    "    df = pd.DataFrame(service_times_log)\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 512),  # Aumenta il numero di neuroni\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 512),  # Aggiungi più strati\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.network(x)\n",
    "\n",
    "def init_weights_he(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "        self.network.apply(init_weights_he)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, To, state_dim, action_dim,max_time):\n",
    "        self.state_dim = state_dim\n",
    "        self.dim = action_dim\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99 #0.99\n",
    "        self.epsilon =1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.03 #0.03\n",
    "        self.total_production_time = To\n",
    "        self.model = DQN(state_dim, action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0001) #0.0001\n",
    "        self.criterion = nn.SmoothL1Loss()#nn.MSELoss() \n",
    "        self.accumulated_rewards = list([])#deque(maxlen=10000)\n",
    "        self.current_time=0\n",
    "        self.max_time= max_time\n",
    "        self.episode_count = 0\n",
    "        self.previous_total_reward = float('inf')\n",
    "        self.rewards_window = deque(maxlen=100)\n",
    "        self.finreward=0\n",
    "\n",
    "    def save_model(self):\n",
    "            torch.save({\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'epsilon': self.epsilon,\n",
    "                'memory': list(self.memory),\n",
    "                'previous_total_reward': self.previous_total_reward # Salva la memoria come lista\n",
    "            }, 'dqn_model.pth')\n",
    "            print(\"Model saved to dqn_model.pth\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        try:\n",
    "            checkpoint = torch.load('dqn_model.pth')\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.epsilon = checkpoint['epsilon']\n",
    "            # print(f'questa è epsilon {self.epsilon}')\n",
    "            self.memory = deque(checkpoint['memory'], maxlen=10000)\n",
    "            self.previous_total_reward = checkpoint.get('previous_total_reward',float('inf') )  # Carica la memoria\n",
    "            self.model.train()  # Imposta il modello in modalità di addestramento\n",
    "            #self.model.eval()  # Imposta il modello in modalità di valutazione\n",
    "            print(\"Model loaded from dqn_model.pth\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No saved model found, starting with a new model\")\n",
    "    \n",
    "\n",
    "    def update_last_transition(self):\n",
    "        if not self.memory:\n",
    "            print(\"Memory is empty, no transition to update.\")\n",
    "            return\n",
    "        \n",
    "        # Trova l'ultimo episodio\n",
    "        last_episode = [] \n",
    "        count = 0\n",
    "        for transition in reversed(self.memory):\n",
    "            count += 1\n",
    "            if transition[4]:  # Se done è True\n",
    "                break\n",
    "            last_episode.insert(0, transition)\n",
    "        \n",
    "        if not last_episode:\n",
    "            print(\"No complete episode found in memory.\")\n",
    "            return\n",
    "        \n",
    "        final_reward = self.calculate_final_reward()\n",
    "        for t in range(len(last_episode)):\n",
    "            state, action, reward, next_state, done = last_episode[t]\n",
    "            discount_factor = self.gamma ** (len(last_episode) - t - 1)\n",
    "            if t == len(last_episode) - 1: \n",
    "                print(f\"questo è reward inserito {final_reward}\")\n",
    "                last_episode[t] = (state, action, final_reward, next_state, True)\n",
    "            else:\n",
    "                discounted_reward=discount_factor * final_reward\n",
    "                last_episode[t] = (state, action, discounted_reward, next_state, done)\n",
    "        for _ in range(len(last_episode)):\n",
    "            self.memory.pop()\n",
    "        self.memory.extend(last_episode)\n",
    "        self.accumulated_rewards.clear()\n",
    "        self.save_model()\n",
    "    \n",
    "\n",
    "    #     # print(\"final_reward\",final_reward)\n",
    "    #     # for transition in last_episode [:-1]:\n",
    "    #     #     final_reward += transition[2]\n",
    "    #     max_reward=0\n",
    "    #     min_reward=(-self.total_production_time-500)\n",
    "        \n",
    "    #     # Aggiorna i reward delle transizioni nell'episodio\n",
    "    #     try:\n",
    "    #         for t in range(len(last_episode)):\n",
    "    #             state, action, reward, next_state, done = last_episode[t]\n",
    "    #             discount_factor = self.gamma ** (len(last_episode) - t - 1)\n",
    "    #             normalized_rewards = 2 * (final_reward - min_reward) / (max_reward - min_reward) - 1\n",
    "    #             if t == len(last_episode) - 1:\n",
    "    #                 print(f\"questo è quanto è lungo episodio {t} -1\")\n",
    "    #                 # normalized_rewards = 2 * (final_reward - min_reward) / (max_reward - min_reward) - 1\n",
    "    #                 last_episode[t] = (state, action, normalized_rewards, next_state, True)\n",
    "    #                 # print(f\"final reward {final_reward}\")\n",
    "    #                 # print(f\"ciao max reward {max_reward}\")\n",
    "    #                 # print(f\" min reeward {min_reward}\")\n",
    "    #                 print(f\"reward finale normalizzato {normalized_rewards}\")\n",
    "    #             else:\n",
    "    #                 # Somma il reward finale scontato al reward intermedio\n",
    "    #                 #normalized_rewards = 2 * (final_reward - min_reward) / (max_reward - min_reward) - 1\n",
    "    #                 discounted_final_reward = discount_factor * normalized_rewards\n",
    "    #                 updated_reward = discounted_final_reward\n",
    "    #                 #normalized_rewards = 2 * (updated_reward - min_reward) / (max_reward - min_reward) - 1\n",
    "    #                 last_episode[t] = (state, action, updated_reward, next_state, done)\n",
    "\n",
    "\n",
    "\n",
    "            # except IndexError as e:\n",
    "            #     print(f\"IndexError encountered: {e}\")\n",
    "            #     return\n",
    "        \n",
    "     \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = np.array(state)\n",
    "        next_state = np.array(next_state)\n",
    "        state = np.nan_to_num(state, nan=9999)\n",
    "        next_state = np.nan_to_num(next_state, nan=9999)\n",
    "        if state is None or action is None or reward is None or next_state is None:\n",
    "            print(\"Invalid entry detected, skipping insertion\")\n",
    "        # if action != -1 or reward is not None:\n",
    "        self.accumulated_rewards.append(reward)\n",
    "        # print(f\"Accumulated rewards: {reward} che aggiungo\")\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "            # else:\n",
    "            # pass\n",
    "\n",
    "\n",
    "    def isdone(self,env):\n",
    "        if env.now >= self.max_time:\n",
    "            # print(\"Simulation finished\")\n",
    "            # self.save_model()\n",
    "            return True\n",
    "        return False\n",
    "       \n",
    "    def calculate_final_reward(self):\n",
    "        # Calcola il makespan come reward\n",
    "        reward_f=self.finreward\n",
    "        # if math.isinf(self.previous_total_reward):\n",
    "        #     self.previous_total_reward = reward_f\n",
    "        # else:\n",
    "        #     if reward_f <= self.previous_total_reward:\n",
    "        #         self.previous_total_reward = reward_f\n",
    "        #         print(f\" il nuovo self.previous_total_reward è: {self.previous_total_reward}\")\n",
    "        #         reward_f += 300\n",
    "        #     else:\n",
    "        #         reward_f -= 100\n",
    "\n",
    "        # Penalizza se il magazzino non è vuoto\n",
    "        if lab.gate.Store.items:\n",
    "            print(\"Store is not empty, penalizing\")\n",
    "            penalty = 500  # Penalità normalizzata\n",
    "            reward_f -= penalty\n",
    "        # print(f\"Makespan: {makespan}, Normalized reward: {normalized_reward}\")\n",
    "        # return normalized_reward\n",
    "        return reward_f\n",
    "   \n",
    "    \n",
    "\n",
    "    \n",
    "        # print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}, Done: {done}\")\n",
    "        # print(f\"Memory before append: {self.memory}\")\n",
    "        # self.memory.append((state, action, reward, next_state, done))\n",
    "        # print(f\"Memory after append: {self.memory}\")\n",
    "        \n",
    "        # if not done:\n",
    "        #     print(f\"Accumulated rewards before: {self.accumulated_rewards}\")\n",
    "        #     if len(self.accumulated_rewards) > 0:\n",
    "        #         self.accumulated_rewards[-1] += reward\n",
    "        #     print(f\"Accumulated rewards after: {self.accumulated_rewards}\")\n",
    "        # else:\n",
    "        #     delayed_reward = self.get_reward(state)\n",
    "        #     print(f\"Delayed reward: {delayed_reward}\")\n",
    "        #     self.accumulated_rewards.append(delayed_reward)\n",
    "        #     print(f\"Accumulated rewards after append: {self.accumulated_rewards}\")\n",
    "    \n",
    "\n",
    "    # def act(self, state): \n",
    "    #     if isinstance(state, pd.DataFrame):\n",
    "    #         # state = state.fillna()  # Riempie i valori NaN con 0\n",
    "    #         state = state.infer_objects(copy=False)\n",
    "    #         type_col_index = state.columns.get_loc('type')\n",
    "    #         entity_index= state.columns.get_loc('Entity')\n",
    "    #         states = state.to_numpy()\n",
    "    #         if np.random.rand() <= self.epsilon:\n",
    "    #             filtered_state = states[state['type'] == 0]\n",
    "    #             ris=random.choice(filtered_state['Entity'].tolist())\n",
    "    #         else:\n",
    "    #             return print(\"ERROR\")\n",
    "    #     q_values= self.model.predict(state)\n",
    "    #     return np.argmax(q_values[0])\n",
    "\n",
    "    def act(self, state, type_col_index, entity_col_index):\n",
    "        #print(f\"colonne indice {entity_col_index}\")\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = np.nan_to_num(state, nan=9999)\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                filtered_state = state[state[:, type_col_index] == 0]\n",
    "                filtered_states=pd.DataFrame(filtered_state)\n",
    "                print(lab.gate.Store.items)\n",
    "                # print(f\"Filtered state:\\n{filtered_states}\")\n",
    "                if filtered_state.size > 0:\n",
    "                    selected_value = np.random.choice(filtered_state[:, entity_col_index])\n",
    "                    # print(selected_value)\n",
    "                    for i in range(len(filtered_state)):\n",
    "                        # print(f\"la lunghezza è {len(filtered_state)}\")\n",
    "                        if math.isclose(state[i][entity_col_index], selected_value, rel_tol=1e-9):\n",
    "                            print(i)  \n",
    "                            return i\n",
    "                 \n",
    "                else:\n",
    "                    i=-1\n",
    "                    return i\n",
    "            else:\n",
    "                # Usa il modello per predire i Q-values e seleziona l'azione con il valore massimo\n",
    "                # print(\"EXPLOIT\")\n",
    "                # print(state.shape)\n",
    "                state_tensor = torch.FloatTensor(state)\n",
    "                q_values = self.model(state_tensor)\n",
    "                max_index=int(np.argmax(q_values.detach().numpy()[0]))\n",
    "                # print(f\"Max index: {max_index}\")\n",
    "                if max_index < 0 or max_index > len(state):\n",
    "                    i=-1\n",
    "                    return i\n",
    "    \n",
    "                return max_index\n",
    "        else:\n",
    "            print(\"State is not a NumPy array\")\n",
    "            return None\n",
    "        \n",
    "    # def update_epsilon(self):\n",
    "    #     # Aggiorna epsilon per ridurre gradualmente l'esplorazione\n",
    "    #     if self.epsilon > self.epsilon_min:\n",
    "    #         self.epsilon *= self.epsilon_decay   \n",
    "    \n",
    "    def get_reward(self, state,colonne,action,flag):\n",
    "        \n",
    "                # Verifica che state sia un array NumPy\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            raise ValueError(\"state deve essere un array NumPy\")\n",
    "\n",
    "        # Verifica che columns sia un array NumPy\n",
    "        if not isinstance(colonne, np.ndarray):\n",
    "            raise ValueError(\"columns deve essere un array NumPy\")\n",
    "        if action==-1 and lab.gate.Store.items:\n",
    "            reward= -100\n",
    "        else:\n",
    "            if action ==-1:\n",
    "                reward= -50\n",
    "            else:\n",
    "                reward=0\n",
    "\n",
    "        # if action==-1 and not lab.gate.Store.items:\n",
    "        #     reward=0\n",
    "        # if action!=-1:\n",
    "        #      reward= 50\n",
    "        try:\n",
    "            time_out_index = np.where(colonne == 'timeOut')[0][0]\n",
    "            time_in_index = np.where(colonne == 'timeIn')[0][0]\n",
    "        except IndexError:\n",
    "            raise ValueError(\"columns deve contenere 'timeOut' e 'timeIn'\")\n",
    "\n",
    "        state[:, time_in_index] = np.nan_to_num(state[:, time_in_index], nan=9999)\n",
    "        \n",
    "        state[:, time_out_index] = np.nan_to_num(state[:, time_out_index], nan=9999)\n",
    "        \n",
    "        max_time_out = np.nanmax(state[:, time_out_index])\n",
    "        min_time_in = np.nanmin(state[:, time_in_index])\n",
    "        # makespan = np.nanmax(state[:, time_out_index]) - np.nanmin(state[:, time_in_index])\n",
    "        if max_time_out == 9999:\n",
    "            valid_times_out = state[:, time_out_index][state[:, time_out_index] != 9999]\n",
    "            if len(valid_times_out) > 0:\n",
    "                max_time_out = np.nanmax(valid_times_out)\n",
    "                # print(f\"Max time out: {max_time_out}\")\n",
    "            else:\n",
    "                # print(\"non ci sono tempi diversi\")\n",
    "                max_time_out = 0\n",
    "        # print(max_time_out)\n",
    "        makespan = max_time_out\n",
    "        # print(f\"Makespan: {makespan}\")\n",
    "        self.finreward= -makespan\n",
    "        reward=self.finreward\n",
    "        # if makespan == 0:\n",
    "        #     reward = 0\n",
    "        #     # print(\"Makespan is 0\")\n",
    "        # else:\n",
    "        #     reward = - makespan\n",
    "        return reward\n",
    "\n",
    "    \n",
    "    # def replay(self, batch_size):\n",
    "    #     if len(self.memory) < batch_size:\n",
    "    #         return \n",
    "    #     priorities = np.array([1.0 if transition[4] else 0.1 for transition in self.memory])\n",
    "    #     probabilities = priorities / priorities.sum()\n",
    "    #     indices = np.random.choice(len(self.memory), batch_size, p=probabilities)\n",
    "    #     minibatch = [self.memory[idx] for idx in indices]\n",
    "    #     # minibatch = random.sample(self.memory, batch_size)\n",
    "    #     for state, action, reward, next_state, done in minibatch:           \n",
    "            \n",
    "    #         next_state=np.nan_to_num(next_state, nan=9999)\n",
    "    #         next_state = torch.FloatTensor(next_state)\n",
    "    #         target = reward\n",
    "    #         # print(f\"reward campionato: {reward}\")\n",
    "    #         if reward is None:\n",
    "    #             print(\"Reward is None\")\n",
    "    #             continue\n",
    "    #         if not done:\n",
    "    #             target = reward + self.gamma * torch.max(self.model(next_state)[0]).item()\n",
    "    #         # Converti state in un tensore PyTorch e aggiungi una dimensione extra\n",
    "    #         state=np.nan_to_num(state, nan=9999)\n",
    "    #         state = torch.FloatTensor(state) \n",
    "    #         # print(state.shape)       \n",
    "    #         # Calcola target_f usando il modello\n",
    "    #         target_f = self.model(state)\n",
    "    #           # Verifica che action sia un intero valido\n",
    "    \n",
    "    #         if isinstance(action, float):\n",
    "    #             action = int(action)\n",
    "    #         # if not isinstance(action, int):\n",
    "    #         #     print(f\"Action sbagliata: {action}\")\n",
    "    #         #     raise ValueError(\"action deve essere un intero\")\n",
    "    #         # print(action)\n",
    "    #         if not isinstance(action, int):\n",
    "    #             # print(f\"Action sbagliata: {action}\")\n",
    "    #             raise ValueError(\"action deve essere un intero\")\n",
    "    #         # print(f\"state shape: {state.shape}\")\n",
    "    #         # print(f\"next_state shape: {next_state.shape}\")\n",
    "    #         # print(f\"target_f shape: {target_f.shape}\")\n",
    "    #         # print(f\"action: {action}\")           \n",
    "    #         # if action <0 or action >= target_f.size(1):   # Modificato da size(1) a size(2)\n",
    "    #         #     raise IndexError(\"action non è un indice valido per target_f\")\n",
    "    #         if action <0 or action >= target_f.size(1): #target is  None \n",
    "    #             pass\n",
    "    #         else:               \n",
    "    #             target_f[0][action] = target\n",
    "\n",
    "            \n",
    "    #         # Ottimizza il modello\n",
    "    #         self.optimizer.zero_grad()\n",
    "    #         loss = self.criterion(target_f, self.model(state))\n",
    "    #         loss.backward()\n",
    "    #         self.optimizer.step()\n",
    "        \n",
    "    #     if self.epsilon > self.epsilon_min:\n",
    "    #         self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        # if len(self.memory) < batch_size * 30:\n",
    "        #     print(\"Not enough samples in memory to replay.\")\n",
    "        #     return\n",
    "\n",
    "        # Trova gli indici degli episodi completi\n",
    "        episode_indices = []\n",
    "        current_episode = []\n",
    "        for transition in self.memory:\n",
    "            current_episode.append(transition)\n",
    "            if transition[4]:  # Se done è True\n",
    "                episode_indices.append(current_episode)\n",
    "                current_episode = []\n",
    "        # len(episode_indices)\n",
    "        if len(episode_indices) < batch_size:\n",
    "            print(len(episode_indices))\n",
    "            print(\"Not enough complete episodes to sample a batch.\")\n",
    "            return\n",
    "\n",
    "        # Assegna priorità decrescente agli episodi in base alla loro posizione nella memoria\n",
    "        priorities = np.linspace(1.0, 0.1, len(episode_indices))\n",
    "        probabilities = priorities / priorities.sum()\n",
    "\n",
    "        # Campiona batch_size episodi completi\n",
    "        sampled_episodes = np.random.choice(len(episode_indices), batch_size, p=probabilities)\n",
    "        minibatch = [episode_indices[idx] for idx in sampled_episodes]\n",
    "        num_transitions = 0  \n",
    "        total_loss=0\n",
    "        for episode in minibatch:\n",
    "           \n",
    "            for state, action, reward, next_state, done in episode:\n",
    "                next_state = torch.FloatTensor(np.nan_to_num(next_state, nan=9999))\n",
    "                state = torch.FloatTensor(np.nan_to_num(state, nan=9999))\n",
    "                action = torch.LongTensor([action])\n",
    "                reward = torch.FloatTensor([reward])\n",
    "\n",
    "                # Calcola il valore target utilizzando la formula di Bellman\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target += self.gamma * torch.max(self.model(next_state)).item()\n",
    "\n",
    "                # Calcola i valori predetti dal modello\n",
    "                current_q_values = self.model(state)\n",
    "\n",
    "                # Clona i valori predetti per aggiornare il valore target per l'azione specifica\n",
    "                target_f = current_q_values.clone().detach()\n",
    "                target_f[0][action] = target\n",
    "\n",
    "                # Calcola la perdita tra i valori predetti e i valori target\n",
    "                loss = self.criterion(current_q_values, target_f)\n",
    "\n",
    "                # Ottimizza il modello\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()  # Aggiorna total_loss con il valore scalare della loss\n",
    "                num_transitions += 1  # Incrementa il contatore delle transizioni\n",
    "            # count+=1\n",
    "            # print(count)\n",
    "        print(f\"Numero di transition {num_transitions}\")\n",
    "        average_loss = total_loss / num_transitions\n",
    "        print(f\"Average loss per episodio del minibatch: {average_loss}\")\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# class Generator(pym.Generator):\n",
    "#     def __init__(self,env,name=None,serviceTime=2,serviceTimeFunction=None):\n",
    "#         super().__init__(env,name,serviceTime,serviceTimeFunction)\n",
    "#         self.count = 0\n",
    "#         self.entities=pd.DataFrame(columns=['id', 'Entity', 'front', 'drill', 'robot', 'camera', 'back', 'press', 'manual'])\n",
    "#     def createEntity(self):\n",
    "#         self.count += 1\n",
    "#         # return Entity()\n",
    "#         e = Entity()#ID=self.count)\n",
    "#         #e.serviceTime = dict()\n",
    "#         e.serviceTime['front'] = 10.52\n",
    "#         e.serviceTime['drill'] = choices([3.5, 8.45, 9.65, 11.94],weights=[5,30,30,35])[0] ## Insert the possibility of skipping this stage\n",
    "#         e.serviceTime['robot'] = choices([0, 81, 105, 108 ,120],weights=[91,3,2,2,2])[0]\n",
    "#         # e.serviceTime['camera'] = choices([3,9,12,18,24],weights=[2,3,1,2,2])[0]\n",
    "#         e.serviceTime['camera'] = 3.5+expovariate(1/7.1)\n",
    "#         e.serviceTime['back'] = choices([3.5,10.57],weights=[0.1,0.9])[0]\n",
    "#         # e.serviceTime['press'] = choices([3,9,15])[0]\n",
    "#         if e.serviceTime['back']>0:\n",
    "#             e.serviceTime['press'] = 3.5+expovariate(1/9.5)\n",
    "#         else:\n",
    "#             e.serviceTime['press'] = 3.5\n",
    "#         e.serviceTime['manual'] = max(np.random.normal(9.2,1),0)\n",
    "#         #print(f\"Entity {self.count} service times: {e.serviceTime}\")\n",
    "#         # self.entities.append(e)\n",
    "#         service_time_dict = {f'serviceTime_{i}': [time] for i, time in enumerate(e.serviceTime)}\n",
    "#         entity_dict = {\n",
    "#             'id': e.ID,\n",
    "#             'Entity': e,\n",
    "#             'front': e.serviceTime['front'],\n",
    "#             'drill': e.serviceTime['drill'],\n",
    "#             'robot': e.serviceTime['robot'],\n",
    "#             'camera': e.serviceTime['camera'],\n",
    "#             'back': e.serviceTime['back'],\n",
    "#             'press': e.serviceTime['press'],\n",
    "#             'manual': e.serviceTime['manual']\n",
    "#         }\n",
    "\n",
    "#         entity_df = pd.DataFrame([entity_dict])\n",
    "#         lab.b_df = pd.concat([lab.b_df, entity_df], ignore_index=True)\n",
    "#         # print(f\"ho aggiunto un'unità al batch:\\n{lab.b_df.iloc[-1]}\")\n",
    "#         # print(\"il nuovo batch è:\")\n",
    "#         # print(lab.b_df)\n",
    "#         # print(lab.b_df.shape[1])\n",
    "#         # self.entities = pd.concat([self.entities, entity_df], ignore_index=True)\n",
    "#         # entity_df = pd.DataFrame([e.ID, e, pd.DataFrame(e.serviceTime)], columns=['id', 'Entity', 'front', 'drill', 'robot', 'camera', 'back', 'press', 'manual'])\n",
    "#         # self.entities = pd.concat([self.entities, entity_df], ignore_index=True)\n",
    "#         return e\n",
    "#     # def get_entities_service_times(self,entities):\n",
    "#     # # Convert the 'serviceTime' column to a DataFrame\n",
    "#     #     service_times_df = pd.DataFrame(entities['serviceTime'].tolist())\n",
    "    \n",
    "#     # # Concatenate the 'id', 'Entity', and the new service times DataFrame\n",
    "#     #     data = pd.concat([entities[['id', 'Entity']], service_times_df], axis=1)\n",
    "#     #     print(data)\n",
    "#     #     return data\n",
    "    \n",
    "class Entity:\n",
    "    _id_counter=0\n",
    "    def __init__(self,ID=None):\n",
    "        if ID is None:\n",
    "            self.ID = Entity._id_counter\n",
    "            Entity._id_counter += 1\n",
    "            # print(f\"sto creando un Entity ID: {self.ID}\")\n",
    "        else:\n",
    "            self.ID = ID\n",
    "        self.rework = False\n",
    "        self.serviceTime = dict()\n",
    "        # self.pt['M3'] = 1\n",
    "\n",
    "    def reset_id_counter():\n",
    "        Entity._id_counter = 0\n",
    "    @property\n",
    "    def require_robot(self):\n",
    "        if self.serviceTime['robot']>0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    @property\n",
    "    def ok(self):\n",
    "        return not (self.rework and self.require_robot)\n",
    "    def done(self):\n",
    "        self.rework = False\n",
    "                \n",
    "class LabServer(pym.Server):\n",
    "    def __init__(self,env,name=None,serviceTime=None,serviceTimeFunction=None):\n",
    "        self.controller = None\n",
    "        # serviceTime = 10\n",
    "        super().__init__(env,name,serviceTime,serviceTimeFunction)\n",
    "    def calculateServiceTime(self,entity=None,attribute='serviceTime'):\n",
    "        if not entity.ok:\n",
    "            return 20 #3.5 ### qua andrà messo 10/20 volte maggiore degli altri processing time?\n",
    "        else:\n",
    "            print(\"service time\")\n",
    "            print(super().calculateServiceTime(entity,attribute))\n",
    "            return super().calculateServiceTime(entity,attribute)\n",
    "    def completed(self):\n",
    "        if self.var.entity.ok:\n",
    "            self.controller.Messages.put(self.name)\n",
    "    T2=Transition(pym.Server.Working, pym.Server.Blocking, lambda self: self.env.timeout(self.calculateServiceTime(self.var.entity)), action = lambda self: self.completed())\n",
    "    T3=Transition(pym.Server.Blocking, pym.Server.Starving, lambda self: self.Next.put(self.var.entity),action=lambda self: [self.var.request.confirm(), self.sm.var.entity.done() if self.sm._name=='robot' else None])\n",
    "\n",
    "def plot_service_times(service_times_log):\n",
    "    df = pd.DataFrame(service_times_log)\n",
    "    df.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "    plt.xlabel('Entity')\n",
    "    plt.ylabel('Service Time')\n",
    "    plt.title('Service Times of Entities')\n",
    "    plt.legend(title='Service Stages')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class Terminator(pym.Terminator):\n",
    "    def __init__(self, env, capacity=np.inf):\n",
    "        super().__init__(env, capacity)\n",
    "        self.controller = None\n",
    "        self.register = list()\n",
    "    def completed(self):\n",
    "        if not self.trigger.triggered:\n",
    "            self.trigger.succeed()\n",
    "    def put(self,item):\n",
    "        self.register.append(self._env.now)\n",
    "        self.controller.Messages.put('terminator')\n",
    "        return super().put(item)\n",
    "    def subscribe(self,item):\n",
    "        self.register.append(self._env.now)\n",
    "        self.controller.Messages.put('terminator')\n",
    "        return super().subscribe(item)\n",
    "class Gate(CHFSM):\n",
    "    def __init__(self,env,tempo,statedim,actiondim,max_time,indicatore):\n",
    "        self.real = True\n",
    "        self.default_request_count = 0\n",
    "        self.capacity = 30 #check se è giusto 40 o 30 come era prima\n",
    "        self.lab = None\n",
    "        self.initialWIP = 12\n",
    "        self.targetWIP = 12\n",
    "        self.request = None\n",
    "        self.episode_count = 0\n",
    "        self.message = env.event()\n",
    "        self.WIP = 0\n",
    "        self.WIPlist = list()\n",
    "        self.dones=False \n",
    "        self.training_step=0\n",
    "        self.dqn_agent = DQNAgent(tempo,state_dim=statedim, action_dim=actiondim,max_time=max_time)\n",
    "        self.dqn_agent.load_model()\n",
    "        self.indicatore=indicatore\n",
    "        # self.count=len(b)\n",
    "        super().__init__(env) #prima era in fondo ai self\n",
    "# # #AGGIUNTA\n",
    "    def concatenate_state_dataframes(self, batch_df, machine_df):\n",
    "        batch_df['type'] = 0  # 0 per batch\n",
    "        machine_df['type'] = 1  # 1 per macchina\n",
    "        dfi= pd.concat([batch_df, machine_df], axis=0, sort=False).reset_index(drop=True)\n",
    "        # dfi.to_excel('dfi.xlsx')\n",
    "        return dfi\n",
    "\n",
    "    def convertdataframe(self,df,flag=True):\n",
    "        mapping = {\n",
    "            \"front\": 1,\n",
    "            \"drill\": 2,\n",
    "            \"robot\": 3,\n",
    "            \"camera\": 4,\n",
    "            \"back\": 5,\n",
    "            \"press\": 6,\n",
    "            \"manual\": 7\n",
    "        }\n",
    "        df[\"ResourceName\"] = df[\"ResourceName\"].map(mapping)\n",
    "\n",
    "        mapping2 = {\n",
    "            \"Working\": 1,\n",
    "            \"Blocking\": 2,\n",
    "            \"Starving\": 3\n",
    "        }  \n",
    "        df[\"StateName\"] = df[\"StateName\"].map(mapping2)\n",
    "        df[\"Entity\"] = df[\"id\"]\n",
    "        \n",
    "        # filtered_df = df[df[\"type\"] == 0]\n",
    "\n",
    "        # # Verifica e conversione della colonna \"id\" se è di tipo float\n",
    "        # if filtered_df[\"id\"].dtype == float:\n",
    "        #     df.loc[df[\"type\"] == 0, \"id\"] = df.loc[df[\"type\"] == 0, \"id\"].astype(int)\n",
    "\n",
    "        # # Verifica e conversione della colonna \"Entity\" se è di tipo float\n",
    "        # if filtered_df[\"Entity\"].dtype == float:\n",
    "        #     df.loc[df[\"type\"] == 0, \"Entity\"] = df.loc[df[\"type\"] == 0, \"Entity\"].astype(int)\n",
    "        \n",
    "        df.to_excel('conversionenumpy.xlsx')\n",
    "        data_array=df.to_numpy()\n",
    "        column_names = df.columns.to_numpy()\n",
    "\n",
    "        # print(column_names)\n",
    "        if flag:\n",
    "            return data_array\n",
    "        else:\n",
    "            return data_array, column_names\n",
    "\n",
    "    \n",
    "    \n",
    "    def filtro(self,val):\n",
    "        return lambda item: item == val\n",
    "    \n",
    "    def indexcolumns(self,column):\n",
    "        type_col_index = list(column).index('type')\n",
    "        entity_col_index = list(column).index('Entity')\n",
    "        return type_col_index, entity_col_index\n",
    "    \n",
    "    def action_to_request(self,indice,current_state):\n",
    "        if indice is None or indice < 0 or indice >= current_state.shape[0] :\n",
    "            # print(f\"Indice negativo: {indice}\")\n",
    "            return None\n",
    "        element=None\n",
    "        for i in range(current_state.shape[0]):\n",
    "            if i == indice:\n",
    "                element=current_state[i, 0]\n",
    "                # print(f\"Elemento selezionato: {element}\")\n",
    "                break\n",
    "                \n",
    "        if element is not None:\n",
    "            row = lab.b_df.loc[lab.b_df['id'] == element]\n",
    "            if not row.empty:\n",
    "                values = row['Entity'].values[0]\n",
    "                return values   \n",
    "            else:\n",
    "        # print(f\"No entity found with id {element}\")\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def get_state(self,b_df,val=True):\n",
    "        machine_df=pd.DataFrame(self.env.log,columns=[\"ResourceName\",\"StateName\",\"timeIn\",\"timeOut\"])\n",
    "        machine_df=machine_df.loc[machine_df.ResourceName.isin([\"front\",\"drill\",\"robot\",\"camera\",\"back\",\"press\",\"manual\"])]\n",
    "        batch_df = b_df[b_df['Entity'].isin(lab.gate.Store.items)] # Filtra solo le righe del batch che sono presenti nello store\n",
    "        spazio_stati=self.concatenate_state_dataframes(batch_df,machine_df)\n",
    "        spazio_stati.to_excel('spazio_stati.xlsx')\n",
    "        spazio_stati,index = self.convertdataframe(spazio_stati,flag=False)\n",
    "        # print(spazio_stati.dtype)\n",
    "        if val:\n",
    "            return spazio_stati\n",
    "        else:\n",
    "            return spazio_stati, index\n",
    "    \n",
    "\n",
    "    # def get_next_state(self,b):\n",
    "    #     machine_df = pd.DataFrame(self.env.log, columns=[\"ResourceName\",\"StateName\",\"timeIn\", \"timeOut\"])\n",
    "    #     machine_df=machine_df.loc[machine_df.ResourceName.isin([\"front\",\"drill\",\"robot\",\"camera\",\"back\",\"press\",\"manual\"])]\n",
    "    #     batch_df = b[b['Entity'].isin(lab.gate.Store.items)]  # Filter only the rows of the batch that are present in the store\n",
    "    #     new_spazio_stati = self.concatenate_state_dataframes(batch_df, machine_df)\n",
    "    #     new_spazio_stati=self.convertdataframe(new_spazio_stati)\n",
    "    #     return new_spazio_stati\n",
    "\n",
    "    # def update_delayed_reward(self, reward):\n",
    "    #     if len(self.accumulated_rewards) > 0:\n",
    "    #         self.accumulated_rewards[-1] += reward # If there are, add the new reward to the last element\n",
    "    #     else:\n",
    "    #         self.accumulated_rewards.append(reward) \n",
    "\n",
    "    # def doneboole(self,env):\n",
    "    #     self.dones=self.dqn_agent.is_done(env)\n",
    "    #     return self.dones\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    # def condition(self, item, scheduled_request):\n",
    "    #     if scheduled_request in self.Store.items:\n",
    "    #         # return item == self.Store.items[scheduled_request]\n",
    "    #         return lab.gate.Store.get(scheduled_request)\n",
    "    #     else:\n",
    "    #         # Gestisci il caso in cui l'indice sia fuori dai limiti\n",
    "    #         print(f\"Indice {scheduled_request} fuori dai limiti\")\n",
    "    #         return False\n",
    "\n",
    "    \n",
    "    # def get(self, condition):\n",
    "    #     for item in self.Store.items:\n",
    "    #         if condition(item):\n",
    "    #             return item\n",
    "    #     return None\n",
    "\n",
    "    # def custom_condition(self, item, scheduled_request):\n",
    "    #     return self.condition(item, scheduled_request)\n",
    "\n",
    "#FINE AGGIUNTA\n",
    "    def build(self):\n",
    "        self.Store = pym.Store(self.env,self.capacity)\n",
    "        self.Messages = pym.Store(self.env)\n",
    "    def put(self,item):\n",
    "        return self.Store.put(item)\n",
    "    class Loading(State):\n",
    "        def _do(self):\n",
    "            # print('Load: %d' %self.sm.initialWIP)\n",
    "            self.sm.initialWIP -= 1\n",
    "            self.fw()\n",
    "    class Waiting(State):\n",
    "        initial_state = True\n",
    "        def _do(self):\n",
    "            self.sm.message = self.Messages.subscribe()\n",
    "            if self.sm.initialWIP > 0:\n",
    "                self.initial_timeout = self.env.timeout(1)\n",
    "            else:\n",
    "                self.initial_timeout = self.env.event()\n",
    "    class Forwarding(State):\n",
    "        def _do(self):\n",
    "            self.message.confirm()\n",
    "            if self.message.value == 'terminator':\n",
    "                self.sm.WIP -= 1\n",
    "                self.sm.WIPlist.append([self.env.now,self.WIP])\n",
    "                # print(self.sm.WIPlist)\n",
    "            self.FIFO()\n",
    "            self.CONWIP()\n",
    "    \n",
    "    def CONWIP(self):\n",
    "        if self.message.value == 'terminator':\n",
    "            self.fw()\n",
    "    def FIFO(self):\n",
    "        pass\n",
    "    def fw(self):\n",
    "        if self.indicatore==True:  \n",
    "            if self.request is None:\n",
    "            #self.request = self.Store.get(self.filtro('<__main__.Entity object at 0x00000222C40317D0>'))\n",
    "    #INIZIO NUOVO CODICE\n",
    "                # count=len(lab.gate.Store.items)\n",
    "                # print(f'numero di entità nello store {count}')\n",
    "                flag=0\n",
    "                current_state,columns = self.get_state(lab.b_df,val=False)\n",
    "                # pd.DataFrame(current_state).to_excel('current_state.xlsx')\n",
    "                type_col_index, entity_col_index = self.indexcolumns(columns)\n",
    "                current_state_df=pd.DataFrame(current_state,columns=columns)\n",
    "        \n",
    "                action=self.dqn_agent.act(current_state,type_col_index, entity_col_index) #index intero\n",
    "                # print(f'azioni selezionate indice{action}')            \n",
    "                # count=len(lab.gate.Store.items)\n",
    "                actionselct = self.action_to_request(action,current_state) #entity\n",
    "               \n",
    "                # print(f'azioni selezionate {actionselct}')\n",
    "                # if lab.gate.Store.items:\n",
    "                \n",
    "                \n",
    "                if actionselct is not None:\n",
    "                    flag=1\n",
    "                    print(f'azioni selezionate {actionselct}')\n",
    "                    self.request = self.Store.get(self.filtro(actionselct))\n",
    "                    print(f\"Request: {self.request}\")\n",
    "                    # print(len(lab.gate.Store.items))\n",
    "                    # print(self.request)\n",
    "                    self.Next.put(self.request.value)\n",
    "                    self.request = None\n",
    "                    self.WIP += 1\n",
    "                    self.WIPlist.append([self.env.now,self.WIP])\n",
    "                \n",
    "                next_state=self.get_state(lab.b_df)\n",
    "                reward = self.dqn_agent.get_reward(current_state,columns,action,flag) \n",
    "                # if actionselct is None :\n",
    "                #     action=-1\n",
    "                # if flag==1 or lab.gate.Store.items:\n",
    "                if lab.gate.Store.items:\n",
    "                    self.dqn_agent.remember(current_state, action, reward, next_state, self.dones)\n",
    "                self.episode_count += 1\n",
    "                if len(self.dqn_agent.memory) > 200:\n",
    "                    if self.episode_count % 5 == 0:\n",
    "                        print(self.episode_count)\n",
    "                        print(\"ciao\")\n",
    "                        self.dqn_agent.replay(20)\n",
    "                        # 10 e 8\n",
    "            else:\n",
    "                print('Error: request is not None')\n",
    "                pass\n",
    "        else:\n",
    "            if self.request is None:\n",
    "                self.request = self.Store.get()\n",
    "            try:\n",
    "                self.Next.put(self.request.value)\n",
    "                self.request = None\n",
    "                self.WIP += 1\n",
    "                self.WIPlist.append([self.env.now,self.WIP])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    \n",
    "             \n",
    "\n",
    "    T0 = Transition(Waiting,Loading,lambda self: self.initial_timeout)\n",
    "    T1 = Transition(Waiting,Forwarding,lambda self: self.sm.message)\n",
    "    T2 = Transition(Loading,Waiting,None)\n",
    "    T3 = Transition(Forwarding,Waiting,None)\n",
    "         \n",
    "\n",
    "class Router(pym.Router):\n",
    "    def __deepcopy(self,memo):\n",
    "        super().deepcopy(self,memo)\n",
    "    def __init__(self, env, name=None):\n",
    "        super().__init__(env, name)\n",
    "        self.var.requestOut = []\n",
    "        self.var.sent = []\n",
    "        self.putEvent = env.event()\n",
    "    def build(self):\n",
    "        self.Queue = Box(self.env)\n",
    "    def condition_check(self,item,target):\n",
    "        return True\n",
    "    def put(self,item):\n",
    "        if self.putEvent.triggered:\n",
    "            self.putEvent.restart()\n",
    "        self.putEvent.succeed()\n",
    "        return self.Queue.put(item)\n",
    "    class Sending(State):\n",
    "        initial_state = True\n",
    "        def _do(self):\n",
    "            self.sm.putEvent.restart()\n",
    "            self.sm.var.requestIn = self.sm.putEvent\n",
    "            self.sm.var.requestOut = [item for sublist in [[next.subscribe(item) for next in self.sm.Next if self.sm.condition_check(item,next)] for item in self.sm.Queue.items] for item in sublist]\n",
    "            if self.sm.var.requestOut == []:\n",
    "                self.sm.var.requestOut.append(self.sm.var.requestIn)\n",
    "    S2S2 = Transition(Sending,Sending,lambda self:AnyOf(self.env,self.var.requestOut),condition=lambda self:self.var.requestOut != [])\n",
    "    def action2(self):\n",
    "        self.Queue._trigger_put(self.env.event())\n",
    "        if not hasattr(self.var.requestOut[0],'item'):\n",
    "            return\n",
    "        for request in self.var.requestOut:\n",
    "            if not request.item in self.Queue.items:\n",
    "                request.cancel()\n",
    "                continue\n",
    "            if request.triggered:\n",
    "                if request.check():\n",
    "                    request.confirm()\n",
    "                    self.Queue.forward(request.item)\n",
    "                    continue\n",
    "    S2S2._action = action2\n",
    "'''  \n",
    "from pymulate import RouterNew\n",
    "class Router(RouterNew):\n",
    "    def __init__(self, env, name=None):\n",
    "        capacity=1\n",
    "        super().__init__(env, name,capacity)\n",
    "'''\n",
    "class RobotSwitch1(Router):\n",
    "    def condition_check(self, item, target):\n",
    "        if item.require_robot:\n",
    "            item.rework = True\n",
    "        if item.require_robot and target.name == 'convRobot1S':\n",
    "            return True\n",
    "        elif not item.require_robot and target.name != 'convRobot1S':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "class RobotSwitch2(Router):\n",
    "    def condition_check(self, item, target):\n",
    "        if len(target.Next)<2:\n",
    "            item.rework = False\n",
    "            return True\n",
    "        else:\n",
    "            item.rework = True\n",
    "            return False    \n",
    "\n",
    "class CloseOutSwitch(Router):\n",
    "    def condition_check(self, item, target):\n",
    "        if item.ok and type(target) == Terminator:\n",
    "            return True\n",
    "        elif not item.ok and type(target) != Terminator:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "# class Conveyor(pym.ParallelServer):\n",
    "#     def __init__(self,env,name=None,serviceTime=None,serviceTimeFunction=None,capacity=1):\n",
    "#         self._capacity = capacity\n",
    "#         serviceTime = capacity*3.5\n",
    "#         super().__init__(env,name,serviceTime,serviceTimeFunction,capacity)\n",
    "class Conveyor(pym.Conveyor):\n",
    "    def __init__(self,env,name=None,capacity=3):\n",
    "        super().__init__(env,name,capacity,0.75)\n",
    "        \n",
    "def newDT():\n",
    "    lab = globals()['lab']\n",
    "    deepcopy(lab)\n",
    "\n",
    "def newEntity(thisSeed=1):\n",
    "    #seed(thisSeed)\n",
    "    np.random.seed(thisSeed)\n",
    "    e = Entity()\n",
    "    e.serviceTime['front'] = 10.52\n",
    "    e.serviceTime['drill'] = choices([30, 40, 50, 20],weights=[5,30,30,35])[0]\n",
    "    e.serviceTime['robot'] = choices([0, 81, 105, 108 ,120],weights=[91,3,2,2,2])[0]\n",
    "    e.serviceTime['camera'] = 3.5+expovariate(1/7.1)\n",
    "    e.serviceTime['back'] = choices([3.5,10.57],weights=[0.1,0.9])[0]\n",
    "    if e.serviceTime['back']>0:\n",
    "        e.serviceTime['press'] = 3.5+expovariate(1/7.5)\n",
    "    else:\n",
    "        e.serviceTime['press'] = 3.5\n",
    "    e.serviceTime['manual'] = max(np.random.normal(9.2,2),0)\n",
    "    return e\n",
    "\n",
    "def batchCreate(seed=1,numJobs=10,return_both=False):\n",
    "    # np.random.seed(seed)\n",
    "    jList = []\n",
    "    complist = []\n",
    "    while len(jList)<numJobs:\n",
    "        e=newEntity(seed)\n",
    "        # num = round(np.random.triangular(1,numJobs/2,numJobs))\n",
    "        # # print(num)\n",
    "        # for i in range(num):\n",
    "        jList.append(e)\n",
    "        entity_info = {\n",
    "                'id': e.ID,\n",
    "                'Entity': e,\n",
    "                'serviceTime': e.serviceTime\n",
    "                }\n",
    "        complist.append(entity_info)\n",
    "        if len(jList)>=numJobs:  # l'ho aggiunto io per evitare che si creino più entità di quelle richieste\n",
    "            break\n",
    "    if return_both:\n",
    "        return jList, complist\n",
    "    else:\n",
    "        return jList\n",
    "\n",
    "class Lab():\n",
    "    def __init__(self,b,maxtime):\n",
    "        conveyTime = 6\n",
    "        self.env = pym.Environment() #crea l'ambiente\n",
    "        # self.g = Generator(self.env) #genera un nuovo pezzo\n",
    "        \n",
    "        self.a=self.env.log  #ambiente stati macchine è un dataframe della lista self.state_log\n",
    "        self.machine_df=pd.DataFrame(self.a,columns=[\"ResourceName\",\"StateName\",\"timeIn\",\"timeOut\"])\n",
    "        self.machine_df=self.machine_df.loc[self.machine_df.ResourceName.isin([\"front\",\"drill\",\"robot\",\"camera\",\"back\",\"press\",\"manual\"])]\n",
    "        self.machine_df.to_excel('machine_df.xlsx')\n",
    "        \n",
    "        b_df = pd.DataFrame(b)\n",
    "        self.b_df=pd.concat([b_df['id'],b_df['Entity'], pd.DataFrame(b_df['serviceTime'].tolist())],axis=1)\n",
    "        self.b_df.to_excel('b_df.xlsx')\n",
    "        self.dim_state=self.b_df.shape[1]+self.machine_df.shape[1]+1 #14\n",
    "        self.dim_action=len(b_df) #come la capacity\n",
    "        self.list = []\n",
    "        service_time_columns = [\"front\", \"drill\", \"robot\", \"camera\", \"back\", \"press\", \"manual\"]\n",
    "        service_time_columns = [col for col in service_time_columns if col in self.b_df.columns]\n",
    "        self.tmax = self.b_df[service_time_columns].sum().sum()\n",
    "        # print(f\"questo è tmax {self.tmax}\")\n",
    "        self.gate = Gate(self.env,tempo=self.tmax,statedim=self.dim_state,actiondim=self.dim_action,max_time=maxtime,indicatore=True) #crea il gate\n",
    "\n",
    "        # DR= despaching rule OR=order release \n",
    "        # self.conv1 = Conveyor(self.env,capacity=3)\n",
    "        self.conv1S = pym.Server(self.env,serviceTime=conveyTime) \n",
    "        self.conv1Q = pym.Queue(self.env,capacity=2)\n",
    "        self.front = LabServer(self.env,'front')\n",
    "        # self.conv2 = Conveyor(self.env,capacity=3)\n",
    "        self.conv2S = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.conv2Q = pym.Queue(self.env,capacity=2)\n",
    "        self.drill = LabServer(self.env,'drill')\n",
    "        # self.conv3 = Conveyor(self.env,capacity=3)\n",
    "        self.conv3S = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.conv3Q = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        \n",
    "        self.switch1 = RobotSwitch1(self.env)\n",
    "        # self.convRobot1 = Conveyor(self.env,'convRobot1',capacity=3)\n",
    "        self.convRobot1S = pym.Server(self.env,serviceTime=conveyTime,name='convRobot1S')\n",
    "        self.convRobot1Q = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        # self.bridge = Conveyor(self.env,capacity=3)\n",
    "        self.bridgeS = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.bridgeQ = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        # self.convRobot2 = Conveyor(self.env,'convRobot2',capacity=3)\n",
    "        self.convRobot2S = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.convRobot2Q = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        self.switch2 = RobotSwitch2(self.env)\n",
    "        # self.convRobot3 = Conveyor(self.env,capacity=3)\n",
    "        self.convRobot3S = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.convRobot3Q = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        self.robot = LabServer(self.env,'robot')\n",
    "        # self.convRobotOut = Conveyor(self.env,capacity=3)\n",
    "        self.convRobotOutS = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.convRobotOutQ = pym.Queue(self.env,capacity=2)\n",
    "        # self.conv5 = Conveyor(self.env,capacity=3)\n",
    "        self.conv5S = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.conv5Q = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        self.camera = LabServer(self.env,'camera')\n",
    "        # self.conv6 = Conveyor(self.env,capacity=3)\n",
    "        self.conv6S = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.conv6Q = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        self.back = LabServer(self.env,'back')\n",
    "        # self.conv7 = Conveyor(self.env,capacity=3)\n",
    "        self.conv7S = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.conv7Q = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        self.press = LabServer(self.env,'press')\n",
    "        # self.conv8 = Conveyor(self.env,capacity=3)\n",
    "        self.conv8S = pym.Server(self.env,serviceTime=conveyTime)\n",
    "        self.conv8Q = pym.Queue(self.env,capacity=2)\n",
    "\n",
    "        self.manual = LabServer(self.env,'manual')\n",
    "        self.outSwitch = CloseOutSwitch(self.env)\n",
    "        self.terminator = Terminator(self.env)\n",
    "        \n",
    "        # self.g.Next = self.gate\n",
    "        self.gate.Next = self.conv1S\n",
    "        \n",
    "        # self.conv1.Next = self.front\n",
    "        self.conv1S.Next = self.conv1Q\n",
    "        self.conv1Q.Next = self.front\n",
    "\n",
    "        self.front.Next = self.conv2S\n",
    "        # self.conv2.Next = self.drill\n",
    "        self.conv2S.Next = self.conv2Q\n",
    "        self.conv2Q.Next = self.drill\n",
    "        self.drill.Next = self.conv3S\n",
    "        self.conv3S.Next = self.conv3Q\n",
    "        self.conv3Q.Next = self.switch1\n",
    "        # self.conv3.Next = self.switch1\n",
    "        \n",
    "        self.switch1.Next = [self.convRobot1S,self.bridgeS]\n",
    "        self.convRobot1S.Next = self.convRobot1Q\n",
    "        self.convRobot1Q.Next = self.switch2\n",
    "\n",
    "        self.switch2.Next = [self.convRobot2S,self.convRobot3S]\n",
    "        self.convRobot2S.Next = self.convRobot2Q\n",
    "        self.convRobot2Q.Next = self.robot\n",
    "\n",
    "        self.convRobot3S.Next = self.convRobot3Q\n",
    "        self.convRobot3Q.Next = self.convRobotOutS\n",
    "\n",
    "        self.robot.Next = self.convRobotOutS\n",
    "        self.convRobotOutS.Next = self.convRobotOutQ\n",
    "\n",
    "        self.convRobotOutQ.Next = self.conv5S\n",
    "        self.bridgeS.Next = self.bridgeQ\n",
    "        self.bridgeQ.Next = self.conv5S\n",
    "\n",
    "        \n",
    "        self.conv5S.Next = self.conv5Q\n",
    "        self.conv5Q.Next = self.camera\n",
    "\n",
    "        self.camera.Next = self.conv6S\n",
    "        self.conv6S.Next = self.conv6Q\n",
    "        self.conv6Q.Next = self.back\n",
    "\n",
    "        self.back.Next = self.conv7S\n",
    "        self.conv7S.Next = self.conv7Q\n",
    "        self.conv7Q.Next = self.press\n",
    "\n",
    "        self.press.Next = self.conv8S\n",
    "        self.conv8S.Next = self.conv8Q\n",
    "        self.conv8Q.Next = self.manual\n",
    "\n",
    "        self.manual.Next = self.outSwitch\n",
    "        self.outSwitch.Next = [self.conv1S,self.terminator]\n",
    "        \n",
    "        for x in [self.front,self.drill,self.robot,self.camera,self.back,self.press,self.manual]:\n",
    "            x.controller = self.gate\n",
    "        self.terminator.controller = self.gate\n",
    "        self.gate.lab = self\n",
    "\n",
    "       \n",
    "\n",
    "    def run(self,Tend):\n",
    "        self.env.run(Tend)\n",
    "        if self.gate.dqn_agent.isdone(self.env):\n",
    "            self.gate.dqn_agent.update_last_transition()\n",
    "            print(\"Operazione completata\")\n",
    "        else:\n",
    "            print(\"Operazione non completata\")\n",
    "        \n",
    "       #return pd.DataFrame(self.env.state_log)\n",
    "        return self.env.state_log\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "Entity.reset_id_counter()\n",
    "# s,b = batchCreate(0,numJobs=10,return_both=True)\n",
    "\n",
    "s,b = batchCreate(0,numJobs=10,return_both=True)\n",
    "makespan=list(pd.read_excel('makespan.xlsx')['Makespan'])\n",
    "seed_value=(int(time.time()+i))\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "for i in range(1):\n",
    "    Entity.reset_id_counter()\n",
    "\n",
    "    model_path = 'dqn_model.pth'\n",
    "    if i == 0:\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Modello esistente trovato al ciclo {i}.\")\n",
    "            os.remove(model_path)\n",
    "        else:\n",
    "            print(f\"Nessun modello esistente trovato al ciclo {i}. Inizio con un nuovo modello.\")     #     \n",
    "    lab=Lab(b,1300)\n",
    "    \n",
    "    # lab.gate.Store.items = [item['Entity'] for item in b]\n",
    "    lab.gate.Store.items= copy.copy(s)\n",
    "    if i > 0:\n",
    "        assert lab.gate.dqn_agent.model is not None, \"Model not loaded correctly\"\n",
    "    \n",
    "    lab.run(1300)\n",
    "    df = pd.DataFrame(lab.env.state_log, columns=[\"Resource\",\"ResourceName\",\"State\",\"StateName\",\"Entity\",\"?\",\"timeIn\",\"timeOut\"])\n",
    "    df= df.loc[df.ResourceName.isin([\"front\",\"drill\",\"robot\",\"camera\",\"back\",\"press\",\"manual\"])]\n",
    "    mks=df.timeOut.max()-df.timeIn.min()\n",
    "    df.to_excel('df.xlsx')\n",
    "    makespan.append(mks)\n",
    "    # Verifica che il modello sia stato salvato correttamente\n",
    "    try:\n",
    "        checkpoint = torch.load('dqn_model.pth')\n",
    "        assert 'model_state_dict' in checkpoint, \"Model state dict not found in checkpoint\"\n",
    "        print(\"Model saved correctly\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model not saved correctly\")\n",
    "\n",
    "\n",
    "mksdf = pd.DataFrame(makespan, columns=['Makespan'])\n",
    "mksdf.to_excel('makespan.xlsx')\n",
    "print(makespan)\n",
    "# Crea il grafico a linee\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(makespan, marker='o', linestyle='-', color='b', label='Makespan')\n",
    "\n",
    "# Aggiungi etichette e titolo\n",
    "plt.xlabel('Iterazione')\n",
    "plt.ylabel('Makespan (secondi)')\n",
    "plt.title('Variazione del Makespan nel Tempo')\n",
    "plt.legend()\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
